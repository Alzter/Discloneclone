{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a01718-3816-4e17-a9c1-fcee55d80229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "from utils import LocalPLM, LocalModelArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b7fc57-9f5b-4111-85c2-a98ccf2a8645",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = LocalModelArguments(\n",
    "    model_name_or_path = \"microsoft/Phi-4-mini-instruct\",\n",
    "    cuda_devices = \"0\",\n",
    "    use_4bit_quantization = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = \"float16\",\n",
    "    use_nested_quant = True,\n",
    "    use_reentrant = True\n",
    ")\n",
    "\n",
    "model = LocalPLM(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5618f3-9450-4995-8713-c47e13282b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../discord-chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4f7014-954a-4837-b1c3-d81b15871c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta, datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def parse_discord_conversation(c : pd.DataFrame) -> pd.DataFrame:\n",
    "    def parse_time_str(time : str) -> datetime:\n",
    "        return datetime.strptime(time,'%Y-%m-%dT%H:%M:%S.%f0%z')\n",
    "\n",
    "    c[\"Date\"] = c[\"Date\"].map(parse_time_str)\n",
    "    c[\"Delay\"] = c[\"Date\"] - c[\"Date\"].shift(1)\n",
    "    c[\"Delay\"] = c[\"Delay\"].fillna( timedelta(seconds=0) )\n",
    "    c = c.dropna(subset=\"Content\").reset_index(drop=True)\n",
    "    return c\n",
    "\n",
    "def parse_discord_conversation_csv(path : str) -> pd.DataFrame:\n",
    "    conversation = pd.read_csv(path)\n",
    "    return parse_discord_conversation(conversation)\n",
    "\n",
    "def get_chat(user : str, path : str) -> pd.DataFrame:\n",
    "    path = path + \"/Direct Messages - \" + user + \" [*.csv\"\n",
    "    conversation = glob.glob(path)\n",
    "\n",
    "    if conversation:\n",
    "        return parse_discord_conversation_csv(conversation[0])\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No conversation(s) found at path {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7a7045-cf17-47a6-b531-64edf3f466dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def split_by_conversations(messages : pd.DataFrame, gap_mins : 50, min_conv_length : int = 5) -> list[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split a Discord conversation history into a list of shorter conversations separated by gap_mins minutes.\n",
    "\n",
    "    Args:\n",
    "        messages (DataFrame): Discord conversation history.\n",
    "        gap_mins (int): How many minutes must have elapsed since the last message for the current message to be treated as the start of a new conversation.\n",
    "        min_conv_length (int): If a conversation has less messages than this, don't include it in the list.\n",
    "        max_conv_length (int): If a conversation has more messages than this, slice it up into chunks of this size.\n",
    "\n",
    "    Returns:\n",
    "        conversations (list[DataFrame]): List of all conversations ordered from least to most recent.\n",
    "    \"\"\"\n",
    "    def is_new_conversation(delay : timedelta, max_delay_mins : int = gap_mins):\n",
    "        \"\"\"\n",
    "        Given a delay between messages, asses whether the delay is sufficient enough\n",
    "        for the message to be considered the start of a new conversation.\n",
    "        \"\"\"\n",
    "        max_delay = timedelta(minutes = max_delay_mins)\n",
    "        return delay > max_delay\n",
    "\n",
    "    def get_conversation_indices(messages : pd.DataFrame) -> list[list[int]]:\n",
    "        \"\"\"\n",
    "        Given a Discord conversation history with a boolean column \"Start\"\n",
    "        denoting the start of a conversation, return a 2D list containing\n",
    "        the indices of all messages grouped by conversation.\n",
    "    \n",
    "        Args:\n",
    "            messages (DataFrame): Discord conversation history with \"Start\" column.\n",
    "        Returns:\n",
    "            conversation_indices (list[list[int]])\n",
    "        \"\"\"\n",
    "        start_indices = messages[messages[\"Start\"] == True].index\n",
    "    \n",
    "        indices = []\n",
    "        for i in range(len(start_indices)):\n",
    "            if i >= len(start_indices) - 1: continue\n",
    "            indices.append(\n",
    "                list(range(start_indices[i], start_indices[i+1]))\n",
    "            )\n",
    "        return indices\n",
    "    \n",
    "    messages[\"Start\"] = messages[\"Delay\"].map(is_new_conversation)\n",
    "    messages.loc[0, \"Start\"] = True\n",
    "    \n",
    "    conversation_indices = get_conversation_indices(messages)\n",
    "    \n",
    "    conversations = []\n",
    "    for indices in conversation_indices:\n",
    "\n",
    "        if len(indices) < min_conv_length: continue\n",
    "        conversation = messages.iloc[indices].reset_index(drop=True)\n",
    "        conversations.append(conversation)\n",
    "\n",
    "    return conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463d1e94-3714-4607-aa72-f6e6d6271f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "glob.glob(DATA_PATH + \"/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9b5d67-36f8-4607-83a4-476054aef1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = get_chat(\"Ben D\", DATA_PATH)\n",
    "c = split_by_conversations(c, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea698fc-5df9-42c4-82d3-99d4757e6d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_header(messages : pd.DataFrame, target_user : str | None = None) -> str:\n",
    "    \"\"\"\n",
    "    Return statistics about a Discord conversation (i.e., users involved, time)\n",
    "    \"\"\"\n",
    "    end_time = messages.iloc[-1].Date\n",
    "\n",
    "    users = messages.Author.unique()\n",
    "    if target_user:\n",
    "        if not target_user in users:\n",
    "            users = np.insert(users, 0, target_user)\n",
    "            \n",
    "    string = \"Conversation between \" + \", \".join(users) + \".\"\n",
    "        \n",
    "    string += \"\\nObtained \" + end_time.strftime(\"%y/%m/%d, %H:%M:%S\") + \".\"\n",
    "\n",
    "    return string\n",
    "\n",
    "def to_string(messages : pd.DataFrame, context : str | None = None, header : bool = True, target_user : str | None = None) -> str:\n",
    "    \"\"\"\n",
    "    Convert a Discord conversation history from DataFrame into a raw string.\n",
    "    \"\"\"\n",
    "    string = \"\"\n",
    "    \n",
    "    if header:\n",
    "        string += get_header(messages, target_user=target_user)\n",
    "\n",
    "    if context: string += \"\\nContext of the conversation:\\n\" + context\n",
    "    \n",
    "    for i, message in messages.iterrows():\n",
    "        string += f\"\\n\\n{message.Author} {message.Date.strftime(\"%H:%M:%S\")}\"\n",
    "        string += f\"\\n{message.Content}\"\n",
    "    \n",
    "    return string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f645ce-adeb-492e-bc15-19325b1e6ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "def gen_prompt(messages : pd.DataFrame, prompt : str, context : str | None = None, context_role : Literal[\"system\", \"user\"] = \"system\") -> str:\n",
    "    \"\"\"\n",
    "    Generate a Chat Template prompt to perform NLP tasks on a Discord conversation history.\n",
    "\n",
    "    Args:\n",
    "        messages (DataFrame): The conversation history.\n",
    "        prompt (str): The system prompt to give the LLM.\n",
    "        context (str, optional): Optional additional information related to the conversation. If provided, aids LLM performance.\n",
    "        context_role (Literal[\"system\", \"user\"]) : Whether to append the context to the system prompt or the conversation history. Adding context to the system prompt usually yields better results. Defaults to \"system\".\n",
    "    \"\"\"\n",
    "    messages = to_string(messages, context= context if context_role == \"user\" else None)\n",
    "\n",
    "    if context_role == \"system\" and context: prompt += f\"\\nContext: {context}.\\nAnswer concisely.\"\n",
    "\n",
    "    prompt = [{\"role\":\"system\",\"content\":prompt}]\n",
    "    \n",
    "    prompt.append({\"role\":\"user\",\"content\":messages})\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987bd2a0-fdec-4713-a4d7-de80791e4b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def understand_conversation(messages : pd.DataFrame, target_user : str, context : str | None = None, context_role : Literal[\"system\", \"user\"] = \"system\") -> dict:\n",
    "    \"\"\"\n",
    "    Use NLP to understand the meaning of a Discord conversation history from a third-person perspective.\n",
    "    \n",
    "    Returns three analyses of the conversation:\n",
    "        - Topic: The topic of the conversation between the users.\n",
    "        - Relationship: The relationship between the target user and other users.\n",
    "        - Interest: The level of interest from the target user in the conversation.\n",
    "\n",
    "    The analysis is done sequentially, from back to front:\n",
    "        1. The personal interest of the target user in the conversation is gauged,\n",
    "        2. The level of interest is used to assess the relationship between the users,\n",
    "        3. The users' relationship is used as context when interpreting the subject of their conversation.\n",
    "    \n",
    "    Args:\n",
    "        messages (DataFrame): The conversation history.\n",
    "        target_user (str): Which user to focus on when analysing the conversation.\n",
    "        context (str, optional): Optional additional information related to the conversation. If provided, aids LLM performance.\n",
    "        context_role (Literal[\"system\", \"user\"]) : Whether to append the context to the system prompt or the conversation history. Adding context to the system prompt usually yields better results. Defaults to \"system\".\n",
    "        \n",
    "    Returns:\n",
    "        understanding (dict): The analysis of the conversation.\n",
    "    \"\"\"\n",
    "    interest_prompt=f\"Read the following conversation and tell me how interested {target_user} sounds in it. Be succinct.\"\n",
    "    interest_prompt = gen_prompt(messages, interest_prompt, context=context, context_role=context_role)\n",
    "    interest = model.generate(interest_prompt,max_new_tokens = 64).text\n",
    "    \n",
    "    relationship_prompt= f\"Read the following conversation history and tell me what you think the relationship is between the users. Answer succinctly.\"\n",
    "    \n",
    "    if context: context += \", \" + interest\n",
    "    else: context = interest\n",
    "    relationship_prompt = gen_prompt(messages, relationship_prompt, context=context, context_role=context_role)\n",
    "    relationship = model.generate(relationship_prompt,temperature=1,max_new_tokens = 128).text\n",
    "    \n",
    "    topic_prompt=\"Read the following conversation history and tell me what was discussed. Answer succinctly.\"\n",
    "    topic_prompt = gen_prompt(messages, topic_prompt, context=relationship + \", \" + interest, context_role=context_role)\n",
    "    topic = model.generate(topic_prompt,temperature=1,max_new_tokens = 128).text\n",
    "\n",
    "    #return f\"Conversation topic:\\n{topic}\\n\\nRelationship between users:\\n{relationship}\\n\\nPersonal interest:\\n{interest}\"\n",
    "    return {\"interest\":interest,\"relationship\":relationship,\"topic\":topic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc4600-4765-4d83-a73b-7461a0a81ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def understand_conversation_pov(messages : pd.DataFrame, target_user : str, context : str | None = None, context_role : Literal[\"system\", \"user\"] = \"system\") -> dict:\n",
    "    \"\"\"\n",
    "    Use NLP to understand the meaning of a Discord conversation history from the perspective of a given user in first-person.\n",
    "    \n",
    "    Returns three analyses of the conversation:\n",
    "        - Topic: The topic of the conversation between the users.\n",
    "        - Relationship: The relationship between the target user and other users.\n",
    "        - Interest: The level of interest from the target user in the conversation.\n",
    "\n",
    "    The analysis is done sequentially, from back to front:\n",
    "        1. The personal interest of the target user in the conversation is gauged,\n",
    "        2. The level of interest is used to assess the relationship between the users,\n",
    "        3. The users' relationship is used as context when interpreting the subject of their conversation.\n",
    "    \n",
    "    Args:\n",
    "        messages (DataFrame): The conversation history.\n",
    "        target_user (str): Which user to focus on when analysing the conversation.\n",
    "        context (str, optional): Optional additional information related to the conversation. If provided, aids LLM performance.\n",
    "        context_role (Literal[\"system\", \"user\"]) : Whether to append the context to the system prompt or the conversation history. Adding context to the system prompt usually yields better results. Defaults to \"system\".\n",
    "        \n",
    "    Returns:\n",
    "        understanding (dict): The analysis of the conversation.\n",
    "    \"\"\"\n",
    "    # Get a string for the name of all other users\n",
    "    other_users = \" and \".join([i for i in messages.Author.unique() if not i == target_user])\n",
    "    \n",
    "    interest_prompt=f\"Your name is {target_user}. Read one of your past text conversations with {other_users} and tell me how interested you were during it. Respond with first person perspective. Be succinct.\"\n",
    "    interest_prompt = gen_prompt(messages, interest_prompt, context=context, context_role=context_role)\n",
    "    interest = model.generate(interest_prompt,max_new_tokens = 64).text\n",
    "    \n",
    "    relationship_prompt= f\"Your name is {target_user}. Read one of your past text conversations with {other_users} and tell me what your relationship is with them. Respond with first person perspective. Be succinct.\"\n",
    "    \n",
    "    if context: context += \", \" + interest\n",
    "    else: context = interest\n",
    "    relationship_prompt = gen_prompt(messages, relationship_prompt, context=context, context_role=context_role)\n",
    "    relationship = model.generate(relationship_prompt,temperature=1,max_new_tokens = 128).text\n",
    "    \n",
    "    topic_prompt=f\"Your name is {target_user}. Read one of your past text conversations with {other_users} and tell me what you were talking about. Respond with first person perspective. Be succinct.\"\n",
    "    topic_prompt = gen_prompt(messages, topic_prompt, context=relationship + \", \" + interest, context_role=context_role)\n",
    "    topic = model.generate(topic_prompt,temperature=1,max_new_tokens = 128).text\n",
    "\n",
    "    #return f\"Conversation topic:\\n{topic}\\n\\nMy relationship with {other_users}:\\n{relationship}\\n\\nMy interest in the conversation:\\n{interest}\"\n",
    "    return {\"interest\":interest,\"relationship\":relationship,\"topic\":topic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedb767e-3daa-40e1-a6b0-a6f0235baf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def understanding_to_string(understanding : dict, other_users : str) -> str:\n",
    "    topic, relationship, interest = understanding[\"topic\"], understanding[\"relationship\"], understanding[\"interest\"]\n",
    "    return f\"Conversation topic:\\n{topic}\\n\\nMy relationship with {other_users}:\\n{relationship}\\n\\nMy interest in the conversation:\\n{interest}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739f0f37-9d8a-4b02-96f9-06ac1ec3e3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_thought(conversation : pd.DataFrame, message_id : int, context=\"context\", tokens : int = 128):\n",
    "    conversation = conversation.reset_index(drop=True)[:message_id + 1] \n",
    "    target_message = conversation.iloc[message_id]\n",
    "    target_text = target_message.Content\n",
    "    target_user = target_message.Author\n",
    "    other_users = \" and \".join([i for i in conversation.Author.unique() if not i == target_user])\n",
    "    \n",
    "    context = understand_conversation_pov(conversation, target_user)\n",
    "\n",
    "    thought_prompt = f\"\"\"\n",
    "Your name is {target_user}. You are in a text conversation with {other_users}.\n",
    "Read the conversation, then tell me what you are thinking as you say:\n",
    "'{target_text}'. Answer in first-person tense. Be succinct.\"\"\".strip()\n",
    "\n",
    "    thought_prompt = gen_prompt(conversation, thought_prompt, context=context)\n",
    "\n",
    "    predicted_thought = model.generate(thought_prompt, temperature=1, max_new_tokens=tokens).text\n",
    "\n",
    "    return predicted_thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80d2e62-406d-4766-ab51-353bb4f9831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def conversation_to_dataset(conversation : pd.DataFrame, target_user : str, batch_size : int = 10, thinking_tokens : int = 0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert a Discord conversation into a supervised chat dataset from the perspective of a given user.\n",
    "    This can be used to predict messages from a given user (i.e., training a model to impersonate you).\n",
    "    \n",
    "    Conversations are split up into smaller batches to reduce the size of each input text.\n",
    "    At the start of each new batch, a summarisation of the previous batch's conversation is given as context.\n",
    "    This helps eliminate loss of semantic meaning when slicing conversations into chunks of arbitrary size.\n",
    "\n",
    "    Optionally, you can allow an LLM to guess what the target user was thinking for each message.\n",
    "    This feature is aimed to improve LLM response precision by getting in the head of the target user.\n",
    "    \n",
    "    Args:\n",
    "        conversation (DataFrame): The conversation to convert.\n",
    "        target_user (str): Which user we're trying to predict the messages of.\n",
    "        batch_size (int, optional): Maximum number of new input messages per sample. Defaults to 10.\n",
    "        thinking_tokens (int, optional): If > 0, predicts the thoughts of the target user for each message using a given number of tokens. Defaults to 0.\n",
    "    \"\"\"\n",
    "    data = {\"content\" : [], \"label\" : []}\n",
    "    \n",
    "    # Get the index of each message\n",
    "    indices = list(conversation.index)\n",
    "\n",
    "    # Slice indices into batches / chunks\n",
    "    chunks = [indices[i:i + batch_size] for i in range(0, len(indices), batch_size)]\n",
    "\n",
    "    # If we want to predict the target user's thoughts\n",
    "    # for each message, we should first gauge what the\n",
    "    # relationship between the users is like for all\n",
    "    # messages in the conversation. We can then use\n",
    "    # this relationship info as context for the\n",
    "    # thought prediction prompt to improve its accuracy.\n",
    "    if thinking_tokens > 0:\n",
    "        full_context = understand_conversation_pov( conversation, target_user=target_user )[\"relationship\"]\n",
    "    \n",
    "    # Create an empty context for now\n",
    "    context_str = None\n",
    "    \n",
    "    # For each batch\n",
    "    for i, indices in enumerate(tqdm(chunks, \"Parsing conversation batches\", position=0)):\n",
    "        start_index = indices[0]\n",
    "\n",
    "        # Get the indices of the target user's messages\n",
    "        user_indices = conversation.iloc[indices]\n",
    "        user_indices = user_indices[user_indices.Author == target_user].index\n",
    "        user_indices = list(user_indices)\n",
    "\n",
    "        # For each user message\n",
    "        for index in tqdm(user_indices, \"Parsing messages\", position=1):\n",
    "            # Get all messages which preceded it in the batch as a string\n",
    "            s = start_index\n",
    "            if start_index == index: s -= 1\n",
    "            inputs = conversation.iloc[s:index]\n",
    "            inputs = to_string(inputs, header=True, context=context_str, target_user=target_user)\n",
    "            \n",
    "            # Get the user message itself as a string\n",
    "            output = conversation.iloc[index:index+1]\n",
    "            output = to_string(output, header=False, target_user=target_user)\n",
    "\n",
    "            # Get the user's thought for the given message\n",
    "            if thinking_tokens > 0:\n",
    "\n",
    "                # We have to get the index of the user's message relative to\n",
    "                # the start of the batch for .iloc[] to work inside the batch\n",
    "                local_index = index - start_index\n",
    "\n",
    "                # Create a context for the user's thought for the message\n",
    "                # using their relationship with the other users + conversation history\n",
    "                thinking_context = full_context + \"\\n\" + context_str if context_str else full_context\n",
    "\n",
    "                # Predict the user's thought for the message\n",
    "                thought = predict_thought( conversation.iloc[indices], local_index, context_str, tokens=thinking_tokens)\n",
    "\n",
    "                # Enclose the thought in <thinking> tags\n",
    "                output = f\"<thinking>{thought}</thinking>\\n\\n\" + output\n",
    "\n",
    "            data['content'].append(inputs)\n",
    "            data['label'].append(output)\n",
    "    \n",
    "            print(\"\\n----\\nIN:\")\n",
    "            print(inputs)\n",
    "            print(\"\\n----\\nOUT:\")\n",
    "            print(output)\n",
    "            print(\"----\")\n",
    "\n",
    "        # At the end of each batch, summarise what was discussed\n",
    "        # to use as the context string for the next batch.\n",
    "        # (Only do this if there are more chunks remaining)\n",
    "        if i < len(chunks) - 1:\n",
    "            context_str = understand_conversation_pov( conversation.iloc[indices], context=context_str, target_user=target_user )[\"topic\"]\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db4a7b7-1faf-41e9-8f8c-4a6171cc6ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_to_dataset(c[0], \"alzter\", batch_size=10, thinking_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0796fb9-62d9-4642-984b-fcfa969259a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def understand_conversations(conversations : list[pd.DataFrame], target_user : str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Super understand_conversation:\n",
    "    Analyses meaning for a series of Discord conversations sequentially.\n",
    "    \"\"\"\n",
    "    # We will generate the context for each conversation\n",
    "    contexts = []\n",
    "\n",
    "    # Each conversation is given the context of the previous\n",
    "    # conversation to recursively build meaning. To start with\n",
    "    # we have zero previous context, so set context_str to None.\n",
    "    context_str = None\n",
    "\n",
    "    # Parse the meaning of each conversation sequentially\n",
    "    for conversation in tqdm(conversations, \"Understanding conversations\"):\n",
    "        context = understand_conversation(conversation, target_user=target_user, context=context_str)\n",
    "        contexts.append(context)\n",
    "\n",
    "        # Give the next conversation the summarised topic of this\n",
    "        # conversation for added context to improve meaning extraction\n",
    "        context_str = f\"Previous discussion: {context[\"topic\"]}\"\n",
    "    \n",
    "    # Restructure contexts from list of dicts -> dict of lists\n",
    "    contexts = pd.DataFrame(contexts).to_dict(orient='list')\n",
    "\n",
    "    return pd.DataFrame(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866f9d73-7d0e-432a-94b3-fbf5b208a513",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
