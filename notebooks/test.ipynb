{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a01718-3816-4e17-a9c1-fcee55d80229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "from utils import LocalPLM, LocalModelArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b7fc57-9f5b-4111-85c2-a98ccf2a8645",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = LocalModelArguments(\n",
    "    model_name_or_path = \"microsoft/Phi-4-mini-instruct\",\n",
    "    cuda_devices = \"0\",\n",
    "    use_4bit_quantization = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = \"float16\",\n",
    "    use_nested_quant = True,\n",
    "    use_reentrant = True\n",
    ")\n",
    "\n",
    "model = LocalPLM(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5618f3-9450-4995-8713-c47e13282b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../discord-chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4f7014-954a-4837-b1c3-d81b15871c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta, datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "    \n",
    "def read_conversation(path : str) -> pd.DataFrame:\n",
    "    def parse_time_str(time : str) -> datetime:\n",
    "        return datetime.strptime(time,'%Y-%m-%dT%H:%M:%S.%f0%z')\n",
    "        \n",
    "    c = pd.read_csv(path)\n",
    "    c[\"Date\"] = c[\"Date\"].map(parse_time_str)\n",
    "    c[\"Delay\"] = c[\"Date\"] - c[\"Date\"].shift(1)\n",
    "    c[\"Delay\"] = c[\"Delay\"].fillna( timedelta(seconds=0) )\n",
    "    return c\n",
    "\n",
    "def get_chat(user : str, path : str) -> pd.DataFrame:\n",
    "    path = path + \"/Direct Messages - \" + user + \" [*.csv\"\n",
    "    conversation = glob.glob(path)\n",
    "\n",
    "    if conversation:\n",
    "        return read_conversation(conversation[0])\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No conversation(s) found at path {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7a7045-cf17-47a6-b531-64edf3f466dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "def split_by_conversations(messages : pd.DataFrame, gap_mins : 50, min_conv_length : int = 5, max_conv_length : int = 30) -> list[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split a Discord conversation history into a list of shorter conversations separated by gap_mins minutes.\n",
    "\n",
    "    Args:\n",
    "        messages (DataFrame): Discord conversation history.\n",
    "        gap_mins (int): How many minutes must have elapsed since the last message for the current message to be treated as the start of a new conversation.\n",
    "        min_conv_length (int): If a conversation has less messages than this, don't include it in the list.\n",
    "        max_conv_length (int): If a conversation has more messages than this, slice it up into chunks of this size.\n",
    "\n",
    "    Returns:\n",
    "        conversations (list[DataFrame]): List of all conversations ordered from least to most recent.\n",
    "    \"\"\"\n",
    "    def is_new_conversation(delay : timedelta, max_delay_mins : int = gap_mins):\n",
    "        \"\"\"\n",
    "        Given a delay between messages, asses whether the delay is sufficient enough\n",
    "        for the message to be considered the start of a new conversation.\n",
    "        \"\"\"\n",
    "        max_delay = timedelta(minutes = max_delay_mins)\n",
    "        return delay > max_delay\n",
    "\n",
    "    def get_conversation_indices(messages : pd.DataFrame) -> list[list[int]]:\n",
    "        \"\"\"\n",
    "        Given a Discord conversation history with a boolean column \"Start\"\n",
    "        denoting the start of a conversation, return a 2D list containing\n",
    "        the indices of all messages grouped by conversation.\n",
    "    \n",
    "        Args:\n",
    "            messages (DataFrame): Discord conversation history with \"Start\" column.\n",
    "        Returns:\n",
    "            conversation_indices (list[list[int]])\n",
    "        \"\"\"\n",
    "        start_indices = messages[messages[\"Start\"] == True].index\n",
    "    \n",
    "        indices = []\n",
    "        for i in range(len(start_indices)):\n",
    "            if i >= len(start_indices) - 1: continue\n",
    "            indices.append(\n",
    "                list(range(start_indices[i], start_indices[i+1]))\n",
    "            )\n",
    "        return indices\n",
    "    \n",
    "    messages[\"Start\"] = messages[\"Delay\"].map(is_new_conversation)\n",
    "    messages.loc[0, \"Start\"] = True\n",
    "    \n",
    "    conversation_indices = get_conversation_indices(messages)\n",
    "    \n",
    "    conversations = []\n",
    "    for indices in conversation_indices:\n",
    "\n",
    "        if len(indices) < min_conv_length: continue\n",
    "\n",
    "        # Slice indices so they don't exceed max_conv_length\n",
    "        indices = [indices[i:i + max_conv_length] for i in range(0, len(indices), max_conv_length)]\n",
    "        \n",
    "        for sub_indices in indices:\n",
    "            if len(sub_indices) < min_conv_length: continue\n",
    "            conversations.append(messages.iloc[sub_indices])\n",
    "\n",
    "    return conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8c40a9-fac2-4e2f-b75b-9635d9d8c23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = get_chat(\"ThisGreenDingo\", DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef4366-e6b2-4e2f-9391-e4d86e493ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = split_by_conversations(c, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea698fc-5df9-42c4-82d3-99d4757e6d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_string(messages : pd.DataFrame, context : str | None = None) -> str:\n",
    "    messages = messages\n",
    "\n",
    "    end_time = messages.iloc[-1].Date\n",
    "    \n",
    "    string = \"Conversation history between \" + \", \".join(messages.Author.unique()) + \".\"\n",
    "    \n",
    "    string += \"\\nObtained \" + end_time.strftime(\"%y/%m/%d, %H:%M:%S\") + \".\"\n",
    "\n",
    "    if context: string += \"\\nContext of the conversation:\\n\" + context\n",
    "    \n",
    "    for i, message in messages.iterrows():\n",
    "        string += f\"\\n\\n{message.Author} {message.Date.strftime(\"%H:%M:%S\")}\"\n",
    "        string += f\"\\n{message.Content}\"\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f645ce-adeb-492e-bc15-19325b1e6ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "def gen_prompt(messages : pd.DataFrame, prompt : str, context : str | None = None, context_role : Literal[\"system\", \"user\"] = \"system\") -> str:\n",
    "    \"\"\"\n",
    "    Generate a Chat Template prompt to perform NLP tasks on a Discord conversation history.\n",
    "\n",
    "    Args:\n",
    "        messages (DataFrame): The conversation history.\n",
    "        prompt (str): The system prompt to give the LLM.\n",
    "        context (str, optional): Optional additional information related to the conversation. If provided, aids LLM performance.\n",
    "        context_role (Literal[\"system\", \"user\"]) : Whether to append the context to the system prompt or the conversation history. Adding context to the system prompt usually yields better results. Defaults to \"system\".\n",
    "    \"\"\"\n",
    "    messages = to_string(messages, context= context if context_role == \"user\" else None)\n",
    "\n",
    "    if context_role == \"system\": prompt += f\"\\nContext: {context}.\\nAnswer concisely.\"\n",
    "\n",
    "    prompt = [{\"role\":\"system\",\"content\":prompt}]\n",
    "    \n",
    "    prompt.append({\"role\":\"user\",\"content\":messages})\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987bd2a0-fdec-4713-a4d7-de80791e4b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def understand_conversation(messages : pd.DataFrame, target_user : str = \"alzter\", context : str | None = None, context_role : Literal[\"system\", \"user\"] = \"system\") -> dict:\n",
    "    \"\"\"\n",
    "    Use NLP to understand the meaning of a Discord conversation history from the perspective of a single user.\n",
    "    \n",
    "    Returns three analyses of the conversation:\n",
    "        - Interest: The level of interest from the target user in the conversation.\n",
    "        - Relationship: The relationship between the target user and other users.\n",
    "        - Topic: The topic of the conversation between the users.\n",
    "    \n",
    "    Args:\n",
    "        messages (DataFrame): The conversation history.\n",
    "        target_user (str): Which user to focus on when analysing the conversation.\n",
    "        context (str, optional): Optional additional information related to the conversation. If provided, aids LLM performance.\n",
    "        context_role (Literal[\"system\", \"user\"]) : Whether to append the context to the system prompt or the conversation history. Adding context to the system prompt usually yields better results. Defaults to \"system\".\n",
    "        \n",
    "    Returns:\n",
    "        understanding (dict): The analysis of the conversation.\n",
    "    \"\"\"\n",
    "    interest_prompt=f\"Read the following conversation and tell me how interested {target_user} sounds in it. Be succinct.\"\n",
    "    interest_prompt = gen_prompt(messages, interest_prompt)\n",
    "    interest = model.generate(interest_prompt,max_new_tokens = 64).text\n",
    "    \n",
    "    relationship_prompt= \"Read the following conversation history and tell me what you think the relationship is between the users. Answer succinctly.\"\n",
    "    \n",
    "    if context: context += \", \" + interest\n",
    "    else: context = interest\n",
    "    relationship_prompt = gen_prompt(messages, relationship_prompt, context=context)\n",
    "    relationship = model.generate(relationship_prompt,temperature=1,max_new_tokens = 128).text\n",
    "    \n",
    "    topic_prompt=\"Read the following conversation history and tell me what was discussed. Answer succinctly.\"\n",
    "    topic_prompt = gen_prompt(messages, topic_prompt, context=relationship + \", \" + interest)\n",
    "    topic = model.generate(topic_prompt,temperature=1,max_new_tokens = 128).text\n",
    "\n",
    "    return {\"interest\":interest,\"relationship\":relationship,\"topic\":topic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0796fb9-62d9-4642-984b-fcfa969259a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def understand_conversations(conversations : list[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Super understand_conversation:\n",
    "    Analyses meaning for a series of Discord conversations sequentially.\n",
    "    \"\"\"\n",
    "    # We will generate the context for each conversation\n",
    "    contexts = []\n",
    "\n",
    "    # Each conversation is given the context of the previous\n",
    "    # conversation to recursively build meaning. To start with\n",
    "    # we have zero previous context, so set context_str to None.\n",
    "    context_str = None\n",
    "\n",
    "    # Parse the meaning of each conversation sequentially\n",
    "    for conversation in tqdm(conversations, \"Understanding conversations\"):\n",
    "        context = understand_conversation(conversation, context=context_str)\n",
    "        contexts.append(context)\n",
    "\n",
    "        # Give the next conversation the summarised topic of this\n",
    "        # conversation for added context to improve meaning extraction\n",
    "        context_str = f\"Previous discussion: {context[\"topic\"]}\"\n",
    "    \n",
    "    # Restructure contexts from list of dicts -> dict of lists\n",
    "    contexts = pd.DataFrame(contexts).to_dict(orient='list')\n",
    "\n",
    "    return pd.DataFrame(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fccda6-827b-4d2e-9b68-f66ae6af6532",
   "metadata": {},
   "outputs": [],
   "source": [
    "c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a8c917-1594-4c12-af2a-3ba80ce3e52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context in system prompt\n",
    "\n",
    "understand_conversation(c[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a69722-6883-4d17-bea7-89ed80906358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context in user message\n",
    "\n",
    "understand_conversation(c[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12de1e4-0c3d-4478-bd07-4a4fe7437548",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = understand_conversations(c[3:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13adef5-bfcd-42c1-878f-fd390f0b0092",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Display full width columns\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Optional: Adjust display width for better layout\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47cae31-f20c-421d-a815-893a4c8ad142",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \" \".join([contexts.iloc[-1][i] for i in contexts.iloc[-1].keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82f9d21-e284-458e-b5d3-d4ad72693c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = c[1 + 3]\n",
    "context = contexts.iloc[1]\n",
    "context = context = \" \".join([context[i] for i in context.keys()])\n",
    "\n",
    "print(\n",
    "    model.generate(\n",
    "        gen_prompt(\n",
    "            message,\n",
    "            \"Read the following conversation history and predict alzter's next message. Start your response with 'alzter'.\",\n",
    "        context = context\n",
    "        ),\n",
    "        temperature=1\n",
    "    ).text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866f9d73-7d0e-432a-94b3-fbf5b208a513",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
